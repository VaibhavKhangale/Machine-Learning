# ğŸ“Š Polynomial & Multi-feature Linear Regression from Scratch

This project demonstrates **Polynomial Regression** and **Multi-feature Linear Regression** implemented from scratch in Python, as well as using **Scikit-Learn**.  
It explores how polynomial features can model non-linear data and how multiple features can be handled using gradient descent.

---

## ğŸ§  What I Learned

- Expanding features for Polynomial Regression  
- The role of feature scaling in gradient descent  
- Implementing gradient descent for multiple features  
- Visualizing polynomial curves vs. linear fits  
- Comparing from-scratch implementations with Scikit-Learn  

---

## âš™ï¸ How It Works

1. **Data Preparation** â€“ Load and preprocess data (scaling, polynomial feature creation)  
2. **Model Initialization** â€“ Set initial weights and bias  
3. **Cost Function** â€“ Mean Squared Error (MSE)  
4. **Gradient Descent** â€“ Update weights to minimize cost  
5. **Polynomial Feature Transformation** â€“ Generate higher-order terms  
6. **Visualization** â€“ Compare predicted curves to original data  

---

## ğŸ“Œ How to Run

```bash
# Clone this repo
git clone https://github.com/VaibhavKhangale/Machine-Learning.git

# Polynomial Regression
cd Machine-Learning/Polynomial Regression

# Open in Jupterlab
jupyter lab FILENAME.ipynb

